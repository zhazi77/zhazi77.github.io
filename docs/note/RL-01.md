---
draft: true
date:
  created: 2025-04-14
categories:
  - Learning
  - Memo
tags:
  - Reinforcement Learning
authors:
  - zhazi
---

# 学习强化学习（一）：基本概念

- 两部分：1. 介绍概念 with 例子， 2. 更正式的介绍 under Markov 决策过程的框架


- 用 A grid-world example 介绍了 state($s$), state space($\mathcal{S}$), action($a$), state transition($s_i \xrightarrow{a} s_j $) 的概念

- state-state 表格形式虽然直观，但表达能力有限，只能表达确定性(deterministic)的情况 

- 条件概率形式能处理随机（stochastic) 的情况。

- 介绍了 Policy 的概念, Policy tells agent what actions to take at state. 所有 state transition 的集合($\pi$)

- 介绍了 state-action(概率) 表格来表示策略，要注意和前面区分，编程时考虑的就是这种？

- 介绍了 Reward 的概念。Reward，约定正数表示鼓励，负数表示惩罚

- Reward 可以理解为一种 人机交互 的手段，通过设置 Reward 来引导智能体学习策略

- 介绍了 state-action(reward) 来表示 Reward(只适用于确定性情况，每个行动的 reward 是确定的)，因此又介绍了条件概率的表示

- The reward depends on the state and action, **but not the next state.**

! 先整一个符号表

! 然后跟一个概念列表

- 介绍了轨迹 trajectory 的概念，A trajectory is a state-action-reward chain.(看起来是一个很重要的数学表达)

- 每个轨迹得到一个 Return，不同的策略会得到不同的轨迹

- 无穷长的轨迹，可能发散。由此介绍了 discount rate($\gamma$)， discounted return 的概念。接近 0，关注近期的 reward，接近 1，关注未来的 reward。

- 介绍了 Episode 和 terminal states 的概念，介绍了 episotic tasks 和 continuing tasks 的概念

- 有限步后结束，得到的轨迹结果称为 episode

- 课程把 episotic tasks 转化为 continuing tasks
  - absorbing state
  - 停止状态不停止，继续执行策略

## Example

### 各种概念的引入

## MDP

### Sets

- State:
- Action:
- Reward:

### Probability distribution

- State transition probability
- Reward probability

### Policy

### Markov property


- Markov decision process becomes Markov process once the policy is given!
