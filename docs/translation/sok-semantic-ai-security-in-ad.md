---
draft: true
date: 
  created: 2025-04-30
categories:
  - Reading
tags:
  - SoK
  - Security for AI
  - Autonomous Driving
authors:
  - zhazi
---

# 翻译：SoK: On the Semantic AI Security in Autonomous Driving 


> 今天来翻点自己喜欢的东西。从做点云对抗第一天起我就对**“物理域攻击”**这一个概念感到困惑，这个名称暗示了其与**“数字域攻击”**的联系，甚至隐含了**数字域攻击可以拓展为物理域攻击**的意思。然而，至少在自动驾驶安全领域，有些工作看起来和数字域上广泛研究的“对抗样本”毫无关联。
> 经典的对抗样本的定义为：“对抗样本是指通过在输入样本中添加微小、人类难以察觉的扰动，导致模型产生错误输出的样本。”，而那些针对神经网络的物理域攻击中，攻击者是通过在物理世界中施加扰动（比如添加物体、改变纹理等）间接影响模型的输入的。难道我们要为了复用数字域上找到的对抗样本，而逆推物理域中的扰动吗？这显然是不合理的，因为攻击者的修改受到物理规律的约束，绝大多数数字域上的对抗样本是物理不可行的。那么我在数字域攻击时考虑物理约束呢？当你这么想的时候，其实你已经否定了数字域上的那批对抗样本，准备制作一批新的对抗样本了！而这思考过程还可以进行下去，我们还要考虑传感器的影响，因此上一批对抗样本又被否定了，我们还要再制作一批新的对抗样本。我们还得考虑输入预处理对对抗样本的影响，天哪！我们一直在制作新的对抗样本。

**TODO: 把今晚的想法整理一下，写一篇博客：对抗攻击反思录（三）：无用的对抗样本**

结论：

1. 对抗样本是无用的，用于制作对抗样本的攻击算法/理论是有用的。

**TODO**
因为物理世界中的修改到达模型输入空间之前至少会受到物理规则、传感器、输入预处理等
难道我们要通过构建对抗样本然后
这里模型的输入是传感器的扫描结果，而不是攻击者施加了扰动的。

事实上，如果只是要实现对自动驾驶系统的攻击，我并不需要考虑系统中的那些深度神经网络，我完全可以一枪崩了激光雷达完事，而这也是一种物理域攻击（攻击从物理域发起），而且也确实有一类攻击(1)，通过激光发射器干扰雷达从而导致系统出错。而糟糕的是，无论是针对传感器的攻击还是针对神经网络的攻击，都被称为“物理域攻击”。这让我无法理解这个研究领域的目标是什么，要实现物理域的攻击，
{ .annotate }

1. 我通常戏称这类方法为“激光捅雷达”，管他背后是什么神经网络，我直接把激光雷达捅穿了还有你神经网络什么事:sweat_smile:。

在我看来，针对系统的攻击完全可以通过攻击其暴露在外的接口（传感器）来实现，而针对隐藏在系统中的模型的对抗攻击则难以实现，也不一定能起到很好的效果。

但如果把它们划到一块我又不能理解，这篇论文中提出的**语义 AI 安全**，某种程度上解决了我的困惑。

!!! info "文献"

    - [SoK: On the Semantic AI Security in Autonomous Driving](https://arxiv.org/abs/2203.05314)

## Abstract

自动驾驶（AD）系统依靠 AI 组件来做出安全和正确的驾驶决策。不幸的是，众所周知，当今的 AI 算法通常容易受到对抗性攻击。但是，要使此类 AI 组件级漏洞在系统级别产生语义影响，它需要解决以下重要语义差距：

1. 从系统级攻击输入空间到 AI 组件级
2. 从 AI 组件级攻击影响到系统级。

在本文中，我们将此类研究空间定义为语义 AI 安全 ，而不是通用 AI 安全。在过去的 5 年里，越来越多的研究工作被用于解决 AD 环境中的此类语义 AI 安全挑战，这种挑战已经开始呈现指数级增长趋势。然而，据我们所知，到目前为止，这个新兴的研究领域还没有全面的系统化。

因此，在本文中，我们对这种不断增长的语义 AD AI 安全研究领域的知识进行了首次系统化。我们总共收集和分析了 53 篇此类论文，并根据对安全领域至关重要的研究方面对其进行了系统分类，例如攻击/防御目标 AI 组件、攻击/防御目标、攻击向量、攻击知识、防御可部署性、防御稳健性和评估方法。我们总结了基于现有 AD AI 安全工作之间的垂直比较和与密切相关领域的安全工作的水平定量比较观察到的 6 个最重大的科学差距。有了这些，我们不仅能够在设计层面提供见解和潜在的未来方向，而且能够在研究目标、方法和社区层面提供见解和潜在的未来方向。为了解决最关键的科学方法层面的差距，我们主动为语义 AD AI 安全研究社区开发了一个开源、统一且可扩展的系统驱动型评估平台，名为 PASS。我们还使用我们实施的平台原型来展示使用代表性语义 AD AI 攻击的此类平台的功能和优势。

## Introduction
自动驾驶（AD）汽车现在已经成为我们日常生活中的现实，各种各样的商用和私人 AD 汽车已经在路上行驶。例如，自动驾驶出租车、公共汽车、卡车、送货车等商业 AD 服务已经公开可用，更不用说数百万辆配备 Autopilot 的特斯拉汽车 。为了在复杂和动态的驾驶环境中实现驾驶自动驾驶，AD 系统设计了一系列 AI 组件来处理核心决策过程，例如感知、定位、预测和规划，这些过程基本上构成了 AD 车辆的“大脑”。然而，这使得这些 AI 组件具有高度的安全关键性，因为它们中的错误会导致各种道路危险，甚至致命的后果。

不幸的是，众所周知，当今的 AI 算法，尤其是深度学习，通常容易受到对抗性攻击。 然而，由于这些人工智能算法只是整个 AD 系统的组成部分，因此人们普遍认识到，这种通用的人工智能组件级漏洞不一定会导致系统级漏洞。 这主要是由于语义差距较大：

1. 从系统级攻击输入空间（例如，添加贴纸、激光射击） 到 AI 组件级的攻击输入空间（例如，图像像素变化），或系统到 AI 语义差距 ，需要克服基本的设计挑战，将 AI 输入空间的成功攻击映射回问题空间， 通常称为逆特征映射问题;

2. 从 AI 组件级攻击影响（例如，错误检测到道路物体）到系统级攻击影响（例如，车辆碰撞），或 AI 到系统的语义差距 ，这也是相当重要的，例如，当错误检测到的物体在远距离进行自动紧急制动时，或者当错误检测可以被后续 AI 模块（如对象跟踪）容忍时。

因此，要使 AI 安全工作在系统级别具有语义意义，它必须明确或隐含地解决这两个一般语义差距。在本文中，我们遵循 Seshia 等人的语义对抗性深度学习概念，将这种研究空间称为语义 AI 安全 （而不是通用 AI 安全）。

在过去的 5 年里，越来越多的研究工作被用于解决 AD 上下文中的上述语义 AI 安全挑战，自 2019 年以来开始呈现指数级增长趋势（图 D）。 然而 ，据我们所知，到目前为止，这个新兴的研究领域还没有全面的系统化。有一些与 AD 安全相关的调查，但它们要么没有关注 AD AI 组件（例如，传感器/硬件、车载网络），要么涉及 AD AI 组件，但没有关注解决上述语义 AI 安全挑战的工作。 由于 （1） 后者在语义上更有意义，因此对 AD 系统更有意义，（2） 现在已经出现了大量的它们（超过 50 个，如图 2 所示）。1） 和 （3） 由于 AD 车辆很重、快速移动并在公共场所运行，因此 AD 环境中的此类攻击具有特别高的安全关键性，我们认为现在是总结现状、趋势以及科学差距、见解和未来研究方向的好时机。

在本文中，我们对不断增长的语义 AD AI 安全研究领域进行了首次知识系统化 （SoK）。我们总共收集和分析了 53 篇此类论文，重点关注自 2017 年第一篇论文出现以来的过去 5 年中在安全、计算机视觉、机器学习、人工智能和机器人领域（公认的）顶级场所发表的论文 （§II-C）。接下来，我们根据对安全领域至关重要的研究方面对它们进行分类，包括目标 AI 组件、攻击/防御目标、攻击向量、攻击知识、防御可部署性、防御稳健性以及评估方法 （§III）。 对于每个研究方面，我们强调观察到的领域/特定问题的设计选择，并总结它们的现状和趋势。

基于系统化，我们总结了观察到的 6 个最重大的科学差距 （§IV）， 这些差距基于现有 AD AI 安全工作之间的纵向定量比较以及与密切相关领域（例如无人机） 的安全工作横向的定量比较。有了这些，我们不仅能够在设计层面（例如，未充分探索的攻击目标和向量）提供见解和潜在的未来方向，而且能够在研究目标和方法层面（例如，普遍缺乏系统级评估）以及社区层面（例如，严重缺乏专门针对安全社区工作的开源）。

在所有这些科学差距中，普遍缺乏系统级评估的差距尤为关键，因为由于 AI 到系统的语义差距 （§IV-A）， 它可能导致系统级的攻击/防御进展毫无意义。为了有效地填补这一空白，我们非常希望社区层面的努力共同构建一个通用的系统级评估基础设施，因为 （1） 构建此类基础设施的工程工作具有通用的设计/实现模式;（2） 在 AD 上下文中，如果使用相同的评估场景和度量计算，系统级评估结果仅具有可比性（因此具有科学意义）。

因此，在本文中，我们通过为语义 AD AI 安全研究社区 （§V 开发一个名为 PASS（A utonomous driving Safety 和 Security 的 Platform）来主动解决这一关键的科学方法级差距。).我们选择以仿真为中心的混合动力设计，利用仿真和真实车辆来平衡保真度、经济性、安全性、灵活性、效率和可再现性之间的权衡。该平台将是完全开源的（将在我们的项目网站上提供），以便研究人员可以共同开发新的接口以适应未来的需求，还可以贡献攻击/防御实现以形成语义 AD AI 安全基准，这可以提高可比性、可重复性，并鼓励开源。我们实施了一个原型，并使用它来展示一个示例用法，该示例使用最流行的 AD AI 攻击类别，即基于摄像头的停止标志检测，使用 45 种不同的系统级场景设置组合（即速度、天气和照明）。我们发现，AI 组件级和 AD 系统级的结果差异很大，并且在常见的驾驶场景中经常相互矛盾，这进一步证明了这种系统级评估基础设施建设工作的必要性和好处。演示可在我们的项目网站上获得 https://sites.google.com/view/cav-sec/pass。

总之，这项工作做出了以下贡献：

- 我们执行了不断增长的语义 AD AI 安全研究领域的第一个 SoK。我们总共收集和分析了 53 篇此类论文，并根据对安全领域至关重要的研究方面对其进行了系统分类，包括目标 AI 组件、攻击/防御目标、攻击向量、攻击知识、防御可部署性、防御稳健性和评估方法。

- 我们总结了根据现有 AD AI 安全工作之间的垂直和密切相关领域的安全工作的水平定量比较观察到的 6 个最重大的科学差距。有了这些，我们不仅能够在设计层面提供见解和潜在的未来方向，而且能够在研究目标、方法和社区层面提供见解和潜在的未来方向。

- 为了解决最关键的科学方法级差距，我们主动为语义 AD AI 安全研究社区开发了一个开源、统一且可扩展的系统驱动评估平台 PASS。我们还使用已实施的原型来展示使用代表性 AD AI 攻击的此类平台的功能和优势。
